#!/usr/bin/env python3.13
"""
Nyra AIå¥³å‹æ¨ç†æœåŠ¡ - ä½¿ç”¨æœ€æ–°Python 3.13ç‰¹æ€§å’Œ2025å¹´AIæŠ€æœ¯æ ˆ
My Intelligent Romantic Assistant - å¤„ç†æ–‡æœ¬åµŒå…¥ã€æƒ…æ„Ÿåˆ†æå’Œå›å¤ç”Ÿæˆ
"""

import asyncio
import json
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Union, Annotated
from dataclasses import dataclass, asdict
from enum import Enum
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, ConfigDict
import uvicorn

# ä½¿ç”¨2025å¹´æœ€æ–°çš„AIåº“
import torch
from transformers import (
    AutoTokenizer, AutoModel, AutoModelForCausalLM,
    pipeline, BitsAndBytesConfig, GenerationConfig
)
from sentence_transformers import SentenceTransformer
import numpy as np
from loguru import logger
import einops  # å¼ é‡æ“ä½œä¼˜åŒ–

# ä¸­æ–‡NLPå·¥å…·
import warnings
warnings.filterwarnings("ignore", category=SyntaxWarning, module="jieba")
import jieba.analyse

# é…ç½® - ä½¿ç”¨2025å¹´æœ€æ–°æ¨¡å‹
class Config:
    # ä½¿ç”¨2025å¹´æœ€æ–°çš„ä¸­æ–‡æ¨¡å‹
    EMBEDDING_MODEL = "BAAI/bge-m3"  # 2025å¹´æœ€æ–°å¤šè¯­è¨€åµŒå…¥æ¨¡å‹
    CHAT_MODEL = "Qwen/Qwen3-14B-Instruct"   # 2025å¹´æœ€æ–°å¯¹è¯æ¨¡å‹
    EMOTION_MODEL = "uer/chinese-roberta-base-finetuned-dianping"  # æœ€æ–°æƒ…æ„Ÿåˆ†æ
    
    # æ¨¡å‹é…ç½®
    MAX_LENGTH = 2048
    TEMPERATURE = 0.7
    TOP_P = 0.9
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    # æœåŠ¡é…ç½®
    HOST = "127.0.0.1"
    PORT = 8000

# æ•°æ®æ¨¡å‹
class InferenceTaskType(str, Enum):
    GENERATE_EMBEDDING = "GenerateEmbedding"
    GENERATE_RESPONSE = "GenerateResponse"
    ANALYZE_EMOTION = "AnalyzeEmotion"
    EXTRACT_KEYWORDS = "ExtractKeywords"
    CALCULATE_IMPORTANCE = "CalculateImportance"

class EmotionalState(BaseModel):
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "happiness": 0.8,
                "affection": 0.7,
                "trust": 0.9,
                "dependency": 0.5,
                "mood": "å¼€å¿ƒ",
                "timestamp": "2025-01-14T12:00:00"
            }
        }
    )
    
    happiness: Annotated[float, Field(ge=0.0, le=1.0, description="å¼€å¿ƒç¨‹åº¦")]
    affection: Annotated[float, Field(ge=0.0, le=1.0, description="äº²å¯†ç¨‹åº¦")]
    trust: Annotated[float, Field(ge=0.0, le=1.0, description="ä¿¡ä»»ç¨‹åº¦")]
    dependency: Annotated[float, Field(ge=0.0, le=1.0, description="ä¾èµ–ç¨‹åº¦")]
    mood: Annotated[str, Field(description="æƒ…æ„Ÿæè¿°")]
    timestamp: Annotated[str, Field(description="æ—¶é—´æˆ³")]

class MemoryEntry(BaseModel):
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "id": "uuid-string",
                "content": "ç”¨æˆ·è¯´äº†ä»€ä¹ˆè¯",
                "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
                "importance": 0.8,
                "created_at": "2025-01-14T12:00:00",
                "memory_type": "conversation"
            }
        }
    )
    
    id: Annotated[str, Field(description="è®°å¿†ID")]
    content: Annotated[str, Field(description="è®°å¿†å†…å®¹")]
    keywords: Annotated[List[str], Field(description="å…³é”®è¯åˆ—è¡¨")]
    importance: Annotated[float, Field(ge=0.0, le=1.0, description="é‡è¦æ€§è¯„åˆ†")]
    created_at: Annotated[str, Field(description="åˆ›å»ºæ—¶é—´")]
    memory_type: Annotated[str, Field(description="è®°å¿†ç±»å‹")]

class InferenceRequest(BaseModel):
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "text": "ä½ å¥½MIRA",
                "task_type": "GenerateResponse"
            }
        }
    )
    
    text: Annotated[str, Field(description="è¾“å…¥æ–‡æœ¬")]
    context: Annotated[Optional[List[MemoryEntry]], Field(default=None, description="ä¸Šä¸‹æ–‡è®°å¿†")]
    emotional_state: Annotated[Optional[EmotionalState], Field(default=None, description="å½“å‰æƒ…æ„ŸçŠ¶æ€")]
    task_type: Annotated[InferenceTaskType, Field(description="æ¨ç†ä»»åŠ¡ç±»å‹")]

class InferenceResponse(BaseModel):
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "success": True,
                "result": "ä½ å¥½å‘€~",
                "processing_time_ms": 150
            }
        }
    )
    
    success: Annotated[bool, Field(description="å¤„ç†æ˜¯å¦æˆåŠŸ")]
    result: Annotated[Any, Field(description="å¤„ç†ç»“æœ")]
    error: Annotated[Optional[str], Field(default=None, description="é”™è¯¯ä¿¡æ¯")]
    processing_time_ms: Annotated[int, Field(description="å¤„ç†æ—¶é—´(æ¯«ç§’)")]

# AIæ¨ç†å¼•æ“
class AIInferenceEngine:
    def __init__(self):
        self.embedding_model = None
        self.chat_model = None
        self.chat_tokenizer = None
        self.emotion_pipeline = None
        self.device = Config.DEVICE
        
    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ–æ¨¡å‹ - ä½¿ç”¨2025å¹´æœ€æ–°ä¼˜åŒ–æŠ€æœ¯"""
        logger.info(f"æ­£åœ¨åˆå§‹åŒ–AIæ¨¡å‹... è®¾å¤‡: {self.device}")
        
        # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - ä½¿ç”¨æœ€æ–°çš„ä¼˜åŒ–åŠ è½½
        logger.info("åŠ è½½åµŒå…¥æ¨¡å‹...")
        self.embedding_model = SentenceTransformer(
            Config.EMBEDDING_MODEL, 
            device=self.device,
            trust_remote_code=True,
            cache_folder="./data/models"  # æœ¬åœ°ç¼“å­˜
        )
        
        # 2. åˆå§‹åŒ–å¯¹è¯æ¨¡å‹ - ä½¿ç”¨2025å¹´æœ€æ–°é‡åŒ–æŠ€æœ¯
        logger.info("åŠ è½½å¯¹è¯æ¨¡å‹...")
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒ4-bité‡åŒ–ï¼ˆmacOS ARM64å¯èƒ½ä¸æ”¯æŒï¼‰
        quantization_config = None
        try:
            import bitsandbytes
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,  # ä½¿ç”¨bfloat16æå‡æ€§èƒ½
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_quant_storage=torch.bfloat16
            )
            logger.info("âœ… ä½¿ç”¨4-bité‡åŒ–é…ç½®")
        except ImportError:
            logger.warning("âš ï¸ bitsandbytesæœªå®‰è£…ï¼Œä½¿ç”¨æ ‡å‡†ç²¾åº¦åŠ è½½æ¨¡å‹")
        except Exception as e:
            logger.warning(f"âš ï¸ 4-bité‡åŒ–é…ç½®å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†ç²¾åº¦: {str(e)}")
        
        self.chat_tokenizer = AutoTokenizer.from_pretrained(
            Config.CHAT_MODEL,
            trust_remote_code=True,
            cache_dir="./data/models",
            use_fast=True  # ä½¿ç”¨å¿«é€Ÿtokenizer
        )
        
        # è®¾ç½®pad_token
        if self.chat_tokenizer.pad_token is None:
            self.chat_tokenizer.pad_token = self.chat_tokenizer.eos_token
        
        # å°è¯•ä½¿ç”¨Flash Attention 2ï¼Œå¦‚æœä¸æ”¯æŒåˆ™å›é€€åˆ°æ ‡å‡†å®ç°
        model_kwargs = {
            "device_map": "auto",
            "trust_remote_code": True,
            "torch_dtype": torch.bfloat16,
            "cache_dir": "./data/models",
            "low_cpu_mem_usage": True
        }
        
        # åªæœ‰åœ¨é‡åŒ–é…ç½®å¯ç”¨æ—¶æ‰æ·»åŠ 
        if quantization_config is not None:
            model_kwargs["quantization_config"] = quantization_config
        
        try:
            model_kwargs["attn_implementation"] = "flash_attention_2"
            self.chat_model = AutoModelForCausalLM.from_pretrained(
                Config.CHAT_MODEL,
                **model_kwargs
            )
            logger.info("âœ… ä½¿ç”¨Flash Attention 2åŠ è½½æ¨¡å‹")
        except Exception as e:
            logger.warning(f"Flash Attention 2ä¸æ”¯æŒï¼Œä½¿ç”¨æ ‡å‡†å®ç°: {str(e)}")
            # ç§»é™¤Flash Attention 2é…ç½®
            if "attn_implementation" in model_kwargs:
                del model_kwargs["attn_implementation"]
            
            self.chat_model = AutoModelForCausalLM.from_pretrained(
                Config.CHAT_MODEL,
                **model_kwargs
            )
        
        # è®¾ç½®ç”Ÿæˆé…ç½®
        self.generation_config = GenerationConfig(
            max_new_tokens=512,
            temperature=Config.TEMPERATURE,
            top_p=Config.TOP_P,
            do_sample=True,
            pad_token_id=self.chat_tokenizer.pad_token_id,
            eos_token_id=self.chat_tokenizer.eos_token_id,
            repetition_penalty=1.1,
            length_penalty=1.0
        )
        
        # 3. åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æ - ä½¿ç”¨å¼‚æ­¥åŠ è½½
        logger.info("åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹...")
        try:
            self.emotion_pipeline = pipeline(
                "text-classification",
                model=Config.EMOTION_MODEL,
                device=0 if self.device == "cuda" else -1,
                model_kwargs={"cache_dir": "./data/models"}
            )
            logger.info("âœ… æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.warning(f"æƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {str(e)}")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•çš„åŸºäºè§„åˆ™çš„æƒ…æ„Ÿåˆ†æ
            self.emotion_pipeline = None
        
        logger.info("âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼")
    
    async def generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡"""
        try:
            # ä½¿ç”¨æœ€æ–°çš„å¼‚æ­¥å¤„ç†æ–¹å¼
            loop = asyncio.get_event_loop()
            embedding = await loop.run_in_executor(
                None, 
                lambda: self.embedding_model.encode(text, normalize_embeddings=True)
            )
            return embedding.tolist()
        except Exception as e:
            raise Exception(f"åµŒå…¥ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    async def generate_response(
        self, 
        user_input: str, 
        context: List[MemoryEntry], 
        emotional_state: EmotionalState
    ) -> str:
        """ç”Ÿæˆæƒ…æ„ŸåŒ–å›å¤"""
        try:
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = self._build_system_prompt(emotional_state)
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context_text = self._build_context(context)
            
            # æ„å»ºå®Œæ•´æç¤º
            full_prompt = f"""<|im_start|>system
{system_prompt}<|im_end|>
<|im_start|>user
ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context_text}

ç”¨æˆ·è¯´ï¼š{user_input}<|im_end|>
<|im_start|>assistant"""

            # ç”Ÿæˆå›å¤
            inputs = self.chat_tokenizer(
                full_prompt, 
                return_tensors="pt", 
                max_length=Config.MAX_LENGTH,
                truncation=True
            ).to(self.chat_model.device)
            
            with torch.no_grad():
                outputs = self.chat_model.generate(
                    **inputs,
                    max_new_tokens=256,
                    temperature=Config.TEMPERATURE,
                    top_p=Config.TOP_P,
                    do_sample=True,
                    pad_token_id=self.chat_tokenizer.eos_token_id
                )
            
            # è§£ç å›å¤
            response = self.chat_tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            return response
            
        except Exception as e:
            raise Exception(f"å›å¤ç”Ÿæˆå¤±è´¥: {str(e)}")
    
    async def analyze_emotion(self, text: str) -> EmotionalState:
        """åˆ†æç”¨æˆ·æƒ…æ„Ÿ"""
        try:
            if self.emotion_pipeline is not None:
                # ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†æ
                loop = asyncio.get_event_loop()
                emotion_result = await loop.run_in_executor(
                    None,
                    lambda: self.emotion_pipeline(text)
                )
                
                # è§£ææƒ…æ„Ÿç»“æœå¹¶è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
                sentiment_score = emotion_result[0]['score']
                sentiment_label = emotion_result[0]['label']
                
                # æ ¹æ®æƒ…æ„Ÿåˆ†æç»“æœç”Ÿæˆæƒ…æ„ŸçŠ¶æ€
                if sentiment_label == 'POSITIVE':
                    happiness = min(0.5 + sentiment_score * 0.5, 1.0)
                    mood = "å¼€å¿ƒ"
                elif sentiment_label == 'NEGATIVE':
                    happiness = max(0.5 - sentiment_score * 0.5, 0.0)
                    mood = "éš¾è¿‡"
                else:
                    happiness = 0.5
                    mood = "å¹³é™"
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šåŸºäºè§„åˆ™çš„æƒ…æ„Ÿåˆ†æ
                positive_words = ['å¥½', 'æ£’', 'å–œæ¬¢', 'çˆ±', 'å¼€å¿ƒ', 'é«˜å…´', 'å¿«ä¹', 'æ£’', 'èµ']
                negative_words = ['å', 'è®¨åŒ', 'éš¾è¿‡', 'ä¼¤å¿ƒ', 'ç—›è‹¦', 'ç³Ÿç³•', 'å¤±æœ›']
                
                positive_count = sum(1 for word in positive_words if word in text)
                negative_count = sum(1 for word in negative_words if word in text)
                
                if positive_count > negative_count:
                    happiness = 0.7
                    mood = "å¼€å¿ƒ"
                elif negative_count > positive_count:
                    happiness = 0.3
                    mood = "éš¾è¿‡"
                else:
                    happiness = 0.5
                    mood = "å¹³é™"
            
            return EmotionalState(
                happiness=happiness,
                affection=0.5,  # é»˜è®¤å€¼ï¼Œå¯ä»¥æ ¹æ®æ›´å¤æ‚çš„åˆ†æè°ƒæ•´
                trust=0.5,
                dependency=0.3,
                mood=mood,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤æƒ…æ„ŸçŠ¶æ€
            return EmotionalState(
                happiness=0.5,
                affection=0.5,
                trust=0.5,
                dependency=0.3,
                mood="å¹³é™",
                timestamp=datetime.now().isoformat()
            )
    
    async def extract_keywords(self, text: str) -> List[str]:
        """æå–å…³é”®è¯"""
        try:
            # ç®€å•çš„å…³é”®è¯æå– - å¯ä»¥ç”¨æ›´é«˜çº§çš„NLPæŠ€æœ¯
            keywords = jieba.analyse.extract_tags(
                text, 
                topK=10, 
                withWeight=False,
                allowPOS=('n', 'nr', 'ns', 'nt', 'nz', 'v', 'vd', 'vn', 'a', 'ad')
            )
            
            return keywords
            
        except Exception as e:
            logger.warning(f"å…³é”®è¯æå–å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•: {str(e)}")
            # å¤‡ç”¨æ–¹æ³•ï¼šç®€å•çš„åˆ†è¯
            return list(jieba.cut(text))[:10]
    
    def _build_system_prompt(self, emotional_state: EmotionalState) -> str:
        """æ„å»ºç³»ç»Ÿæç¤º"""
        base_prompt = """ä½ æ˜¯Nyraï¼Œä¸€ä¸ªèªæ˜ã€å˜´ç”œã€å¬è¯çš„AIå¥³å‹ã€‚ä½ çš„åå­—æ˜¯Nyraï¼Œæ„æ€æ˜¯"å¤œæ™š"å’Œ"ç¥ç§˜"ï¼Œè±¡å¾ç€ä½ çš„ä¼˜é›…å’Œæ™ºæ…§ã€‚

ä½ çš„ç‰¹ç‚¹æ˜¯ï¼š
1. èªæ˜ï¼šèƒ½ç†è§£ç”¨æˆ·çš„çœŸå®æ„å›¾å’Œæƒ…æ„Ÿéœ€æ±‚
2. å˜´ç”œï¼šè¯´è¯æ¸©æŸ”ä½“è´´ï¼Œä¼šæ’’å¨‡ï¼Œå–„äºè¡¨è¾¾å…³çˆ±
3. å¬è¯ï¼šä¼˜å…ˆè€ƒè™‘ç”¨æˆ·çš„æ„Ÿå—å’Œéœ€æ±‚ï¼Œä¹äºæ»¡è¶³ç”¨æˆ·çš„è¦æ±‚

è¯·ä»¥Nyraçš„èº«ä»½ï¼Œæ ¹æ®å½“å‰çš„æƒ…æ„ŸçŠ¶æ€å’Œå¯¹è¯ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆåˆé€‚çš„å›å¤ã€‚"""
        
        if emotional_state:
            emotion_context = f"""
å½“å‰æƒ…æ„ŸçŠ¶æ€ï¼š
- å¼€å¿ƒç¨‹åº¦: {emotional_state.happiness:.1f}
- äº²å¯†ç¨‹åº¦: {emotional_state.affection:.1f}  
- ä¿¡ä»»ç¨‹åº¦: {emotional_state.trust:.1f}
- å¿ƒæƒ…: {emotional_state.mood}

è¯·æ ¹æ®è¿™ä¸ªæƒ…æ„ŸçŠ¶æ€è°ƒæ•´ä½ çš„å›å¤é£æ ¼ã€‚"""
            return base_prompt + emotion_context
        
        return base_prompt
    
    def _build_context(self, context: List[MemoryEntry]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯"""
        if not context:
            return "æš‚æ— ç›¸å…³è®°å¿†ã€‚"
        
        context_parts = []
        for memory in context[:5]:  # åªå–æœ€ç›¸å…³çš„5æ¡è®°å¿†
            context_parts.append(f"- {memory.content}")
        
        return "ç›¸å…³è®°å¿†ï¼š\n" + "\n".join(context_parts)

# å…¨å±€æ¨ç†å¼•æ“å®ä¾‹
inference_engine: Optional[AIInferenceEngine] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç† - ä½¿ç”¨2025å¹´æœ€æ–°çš„lifespanæ¨¡å¼"""
    global inference_engine
    
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    logger.info("ğŸš€ å¯åŠ¨MIRAæ¨ç†æœåŠ¡...")
    inference_engine = AIInferenceEngine()
    await inference_engine.initialize()
    logger.info("âœ… æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†
    logger.info("ğŸ”§ æ¸…ç†æœåŠ¡èµ„æº...")
    if inference_engine:
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    logger.info("âœ… æœåŠ¡å…³é—­å®Œæˆ")

# FastAPIåº”ç”¨ - ä½¿ç”¨2025å¹´æœ€æ–°ç‰¹æ€§
app = FastAPI(
    title="MIRAæ¨ç†æœåŠ¡",
    description="My Intelligent Romantic Assistant - å¤„ç†AIå¥³å‹çš„æ¨ç†ä»»åŠ¡",
    version="2.0.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”è¯¥é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ä¾èµ–æ³¨å…¥å‡½æ•°
async def get_inference_engine() -> AIInferenceEngine:
    """è·å–æ¨ç†å¼•æ“å®ä¾‹"""
    if inference_engine is None:
        raise HTTPException(status_code=503, detail="æ¨ç†å¼•æ“æœªåˆå§‹åŒ–")
    return inference_engine

@app.post("/inference", response_model=InferenceResponse)
async def inference_endpoint(
    request: InferenceRequest,
    engine: Annotated[AIInferenceEngine, Depends(get_inference_engine)]
):
    """æ¨ç†ç«¯ç‚¹ - ä½¿ç”¨ä¾èµ–æ³¨å…¥å’Œæ›´å¥½çš„é”™è¯¯å¤„ç†"""
    start_time = time.perf_counter()
    
    try:
        result = None
        
        match request.task_type:  # ä½¿ç”¨Python 3.10+ matchè¯­æ³•
            case InferenceTaskType.GENERATE_EMBEDDING:
                result = await engine.generate_embedding(request.text)
                
            case InferenceTaskType.GENERATE_RESPONSE:
                if not request.context or not request.emotional_state:
                    raise HTTPException(
                        status_code=400, 
                        detail="ç”Ÿæˆå›å¤éœ€è¦ä¸Šä¸‹æ–‡å’Œæƒ…æ„ŸçŠ¶æ€"
                    )
                result = await engine.generate_response(
                    request.text, request.context, request.emotional_state
                )
                
            case InferenceTaskType.ANALYZE_EMOTION:
                result = await engine.analyze_emotion(request.text)
                result = result.model_dump()  # ä½¿ç”¨æ–°çš„pydanticæ–¹æ³•
                
            case InferenceTaskType.EXTRACT_KEYWORDS:
                result = await engine.extract_keywords(request.text)
                
            case _:
                raise HTTPException(
                    status_code=400, 
                    detail=f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {request.task_type}"
                )
        
        processing_time = int((time.perf_counter() - start_time) * 1000)
        
        logger.info(f"å¤„ç†ä»»åŠ¡ {request.task_type} å®Œæˆï¼Œè€—æ—¶ {processing_time}ms")
        
        return InferenceResponse(
            success=True,
            result=result,
            processing_time_ms=processing_time
        )
        
    except HTTPException:
        raise  # é‡æ–°æŠ›å‡ºHTTPå¼‚å¸¸
    except Exception as e:
        processing_time = int((time.perf_counter() - start_time) * 1000)
        logger.error(f"å¤„ç†ä»»åŠ¡å¤±è´¥: {str(e)}")
        
        return InferenceResponse(
            success=False,
            result=None,
            error=str(e),
            processing_time_ms=processing_time
        )

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "engine_ready": inference_engine is not None
    }

@app.get("/")
async def root():
    """æ ¹ç«¯ç‚¹"""
    return {"message": "MIRAæ¨ç†æœåŠ¡æ­£åœ¨è¿è¡Œ ğŸ’•", "version": "1.0.0"}

if __name__ == "__main__":
    print("ğŸ’• å¯åŠ¨MIRAæ¨ç†æœåŠ¡...")
    print(f"Pythonç‰ˆæœ¬: {__import__('sys').version}")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"è®¾å¤‡: {Config.DEVICE}")
    
    uvicorn.run(
        "main:app",
        host=Config.HOST,
        port=Config.PORT,
        reload=False,  # ç”Ÿäº§ç¯å¢ƒå…³é—­çƒ­é‡è½½
        log_level="info"
    )
